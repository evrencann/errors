{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, TensorBoard\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_path = '/home/evren/classified_data/clean_data/'\n",
    "noisy_15dB_path = '/home/evren/classified_data/noisy_data/15dB/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "datagen_clean = ImageDataGenerator(\n",
    "    \n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.1,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=False,  \n",
    "    vertical_flip=False, \n",
    "    rescale = 1./255,\n",
    "    preprocessing_function=None,\n",
    "    data_format='channels_last', # means that the images should have shape (samples, height, width, channels)\n",
    "    validation_split=0.0,\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "datagen_noisy = ImageDataGenerator(\n",
    "    \n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.1,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=False,  \n",
    "    vertical_flip=False, \n",
    "    rescale = 1./255,\n",
    "    preprocessing_function=None,\n",
    "    data_format='channels_last', # means that the images should have shape (samples, height, width, channels)\n",
    "    validation_split=0.0,\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69071 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "clean_data_gen = datagen_clean.flow_from_directory(\n",
    "                clean_data_path,\n",
    "                target_size=(64,128),\n",
    "                color_mode='grayscale',\n",
    "                classes=None,\n",
    "                class_mode='input', # \"input\" will be images identical to input images (mainly used to work with autoencoders)\n",
    "                batch_size=69071,\n",
    "                shuffle=True, \n",
    "                seed=42,\n",
    "                save_to_dir=None,\n",
    "                save_prefix='augmented_',\n",
    "                save_format='png',\n",
    "                follow_links=False,\n",
    "                subset=None,\n",
    "                interpolation='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69071 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "noisy_15dB_gen = datagen_noisy.flow_from_directory(\n",
    "                noisy_15dB_path,\n",
    "                target_size=(64,128),\n",
    "                color_mode='grayscale',\n",
    "                classes=None,\n",
    "                class_mode='input', # \"input\" will be images identical to input images (mainly used to work with autoencoders)\n",
    "                batch_size=69071,\n",
    "                shuffle=True, \n",
    "                seed=42,\n",
    "                save_to_dir=None,\n",
    "                save_prefix='augmented_',\n",
    "                save_format='png',\n",
    "                follow_links=False,\n",
    "                subset=None,\n",
    "                interpolation='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69071\n",
      "69071\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(noisy_15dB_gen.batch_size)\n",
    "print(clean_data_gen.batch_size)\n",
    "steps_per_epoch = 69071 // noisy_15dB_gen.batch_size\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "data_list_15dB = []\n",
    "batch_index = 1\n",
    "\n",
    "while batch_index <= steps_per_epoch:\n",
    "    data = noisy_15dB_gen.next()\n",
    "    data_list_15dB.append(data[0])\n",
    "    print(batch_index)    \n",
    "    batch_index = batch_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data_array_15dB = np.asarray(data_list_15dB, dtype=np.float32)\n",
    "\n",
    "print(len(data_list_15dB))\n",
    "print(len(data_array_15dB))\n",
    "del data_list_15dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69071, 64, 128, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_array_15dB[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "data_list_clean = []\n",
    "batch_index = 1\n",
    "\n",
    "while batch_index <= steps_per_epoch:\n",
    "    data = clean_data_gen.next()\n",
    "    data_list_clean.append(data[0])\n",
    "    print(batch_index)    \n",
    "    batch_index = batch_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data_array_clean = np.asarray(data_list_clean, dtype=np.float32)\n",
    "\n",
    "print(len(data_list_clean))\n",
    "print(len(data_array_clean))\n",
    "del data_list_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_array_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 64\n",
    "width = 128\n",
    "channel = 1\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "  # encoder\n",
    "    Conv2D(8, (3,3), padding=\"same\", activation=\"relu\", input_shape=(height,width,channel)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2),strides=(2,2)),\n",
    "    \n",
    "    \n",
    "    # decoder\n",
    "    Conv2D(8, (3,3), padding=\"same\", activation=\"relu\"),\n",
    "    UpSampling2D((2,2)),\n",
    "    \n",
    "    Conv2D(channel, (3,3), padding=\"same\", activation=\"sigmoid\") # sigmoid, relu    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 128, 8)        80        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 128, 8)        32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 64, 128, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 128, 1)        73        \n",
      "=================================================================\n",
      "Total params: 769\n",
      "Trainable params: 753\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69071 samples\n",
      "Epoch 1/5\n",
      "   32/69071 [..............................] - ETA: 9:22"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-16badce0f83d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(x, y,\n\u001b[0;32m----> 2\u001b[0;31m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                    )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# If x is a dataset, generator, or keras.utils.Sequence instance,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/evren/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/home/evren/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/evren/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/evren/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.pyc\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/evren/.local/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/evren/.local/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    606\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         raise ValueError(\"Creating variables on a non-first call to a function\"\n\u001b[0m\u001b[1;32m    609\u001b[0m                          \" decorated with tf.function.\")\n\u001b[1;32m    610\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y,\n",
    "                    epochs=5,\n",
    "                   )\n",
    "\n",
    "# If x is a dataset, generator, or keras.utils.Sequence instance, \n",
    "# y should not be specified (since targets will be obtained from x). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (17,6)\n",
    "fig, ax1 = plt.subplots()\n",
    "xepochs = range(1,len(history.history['loss']) + 1)\n",
    "\n",
    "plt.plot(xepochs, history.history['loss'], label = 'loss')\n",
    "# plt.plot(xepochs, history.history['val_loss'], label = 'val_loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(xepochs, history.history['accuracy'], label = 'accuracy')\n",
    "# plt.plot(xepochs, history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.title('Metrics (Accuracy)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_20dB_path = '/home/evren/classified_data/noisy_data/20dB/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_20dB_gen = datagen_clean.flow_from_directory(\n",
    "                noisy_20dB_path,\n",
    "                target_size=(64,128),\n",
    "                color_mode='grayscale',\n",
    "                classes=None,\n",
    "                class_mode='input', # \"input\" will be images identical to input images (mainly used to work with autoencoders)\n",
    "                batch_size=69071,\n",
    "                shuffle=True, \n",
    "                seed=42,\n",
    "                save_to_dir=None,\n",
    "                save_prefix='augmented_',\n",
    "                save_format='png',\n",
    "                follow_links=False,\n",
    "                subset=None,\n",
    "                interpolation='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel died while predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_20dB = model.predict(noisy_20dB_gen)\n",
    "decoded_clean = model.predict(clean_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,4))\n",
    "for i in range(1,7):\n",
    "    ax = plt.subplot(1,7,i)\n",
    "    plt.imshow(noisy_20dB_gen[i])\n",
    "plt.show()\n",
    "\n",
    "# Displaying prediction_train images\n",
    "plt.figure(figsize=(17,4))\n",
    "for i in range(1,7):\n",
    "    ax = plt.subplot(1,7,i)\n",
    "    plt.imshow(decoded_20dB[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(original, compressed): \n",
    "    mse = np.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse)) \n",
    "    return psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'15dB PSNR mean: {PSNR(noisy_15dB, decoded_15dB)}')\n",
    "\n",
    "for i in range(len(noisy_15dB)):\n",
    "    \n",
    "    value = PSNR(noisy_15dB[i], decoded_15dB[i]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
